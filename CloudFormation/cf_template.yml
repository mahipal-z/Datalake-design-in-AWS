Parameters:
  SourceBucketName:
    Type: String
    Description: Name of the source S3 bucket that contains the files to copy

Resources:
  RawS3Bucket:
    Type: 'AWS::S3::Bucket'
    DeletionPolicy: Delete
    Properties:
      BucketName: 'datalake-raw-s3-mz'
    
  ProcessedS3Bucket:
    Type: 'AWS::S3::Bucket'
    DeletionPolicy: Delete
    Properties:
      BucketName: 'datalake-processed-s3-mz'
    
  TransformedS3Bucket:
    Type: 'AWS::S3::Bucket'
    DeletionPolicy: Delete
    Properties:
      BucketName: 'datalake-transformed-s3-mz'
     
  CopyLambdaExecutionRole:
    Type: 'AWS::IAM::Role'
    Properties:
      RoleName: copy-lambda-execution-role
      AssumeRolePolicyDocument:
        Version: '2012-10-17'
        Statement:
          - Effect: Allow
            Principal:
              Service: lambda.amazonaws.com
            Action: 'sts:AssumeRole'
      Policies:
        - PolicyName: copy-lambda-execution-policy
          PolicyDocument:
            Version: '2012-10-17'
            Statement:
              - Effect: Allow
                Action:
                  - 'logs:CreateLogGroup'
                  - 'logs:CreateLogStream'
                  - 'logs:PutLogEvents'
                Resource: 'arn:aws:logs:*:*:*'
              - Effect: Allow
                Action:
                  - 's3:ListBucket'
                  - 's3:GetObject'
                  - 's3:PutObject'                
                Resource:
                  - !Sub 'arn:aws:s3:::${SourceBucketName}/*'
                  - !Sub 'arn:aws:s3:::${SourceBucketName}'
                  - !GetAtt RawS3Bucket.Arn
                  - !Sub '${RawS3Bucket.Arn}/*'
                  
        - PolicyName: 'lambda-service-policy'
          PolicyDocument:
            Version: '2012-10-17'
            Statement:
              - Effect: 'Allow'
                Action: 'glue:StartJobRun'
                Resource: 
                  - !Sub 'arn:aws:glue:${AWS::Region}:${AWS::AccountId}:job/${CombineGlueJob}'
              - Effect: 'Allow'
                Action: 'glue:GetJobRun'
                Resource: 
                  - !Sub 'arn:aws:glue:${AWS::Region}:${AWS::AccountId}:job/${CombineGlueJob}'
              - Effect: 'Allow'
                Action: 'glue:StartJobRun'
                Resource: 
                  - !Sub 'arn:aws:glue:${AWS::Region}:${AWS::AccountId}:job/${TransformGlueJob}'

  CopyLambdaFunction:
    Type: 'AWS::Lambda::Function'
    DependsOn:
      - CopyLambdaExecutionRole
      - RawS3Bucket
    Properties:
      FunctionName: copy-lambda-function
      Handler: index.lambda_handler
      Role: !GetAtt CopyLambdaExecutionRole.Arn
      Environment:
        Variables:
          RawBucketName: !Ref RawS3Bucket
          SourceName: !Ref SourceBucketName
          ProcessedBucketName: !Ref ProcessedS3Bucket
          TransformedBucketName: !Ref TransformedS3Bucket
          GlueJobName: !Ref CombineGlueJob
          GlueJobName2: !Ref TransformGlueJob
      Code:
        ZipFile: |
          import boto3
          import os          
          import time
          
          s3 = boto3.client('s3')          
          glue = boto3.client('glue')
          def lambda_handler(event, context):
              #print(event)
              #to get source bucket name from s3 notification event dictionary
              #source_bucket = event['Records'][0]['s3']['bucket']['name']
              
              
              #to get source bucket name using environment variable
              source_bucket = os.environ['SourceName']  
              source_key = event['Records'][0]['s3']['object']['key']
              print('source_key={}'.format(source_key))                  
              src_folder = os.path.dirname(source_key)
              print('src_folder={}'.format(src_folder))
              src_prefix = src_folder + '/'
              dest_prefix = src_folder + '/'                           
              destination_bucket = os.environ['RawBucketName']      
                          
                           
              # Code to parse parameters defined in the config file
              #if source_key.endswith('run-configuration.config'):
              #  # Get the contents of the config file from S3
              #  config_object = s3.get_object(Bucket=source_bucket, Key=source_key)
              #  config_data = config_object['Body'].read().decode('utf-8')
              #  
              #  # Parse the config file data into a dictionary
              #  config_params = {}
              #  for line in config_data.splitlines():
              #    param, value = line.split('=')
              #    config_params[param.strip()] = value.strip()
              #  print(config_params)          

              src_objects = s3.list_objects(Bucket=source_bucket, Prefix=src_prefix)

              # Check if folder with same name exists in destination bucket and delete it
              existing_objects = s3.list_objects_v2(Bucket=destination_bucket, Prefix=dest_prefix)
              if 'Content' in existing_objects:
                delete_keys = [{'Key': obj['Key']} for obj in existing_objects['Contents']]
                s3.delete_objects(Bucket=destination_bucket, Delete={'Objects': delete_keys})

              if 'Contents' in src_objects:
                for obj in src_objects['Contents']:
                  if obj['Key'].endswith('.csv'):
                    key = obj['Key'].split('/')[-1]
                    src_key = obj['Key']
                    dest_key = os.path.join(dest_prefix, key)
                    s3.copy_object(Bucket=destination_bucket, CopySource={'Bucket': source_bucket, 'Key': src_key}, Key=dest_key)

              # Trigger the Glue jobs to combine the CSV files and convert to parquet
              glue_job_name = os.environ['GlueJobName']
              glue_job_name2 = os.environ['GlueJobName2']
              processed_bucket = os.environ['ProcessedBucketName']
              transformed_bucket = os.environ['TransformedBucketName']
              glue_start_job_args = {
                  'JobName': glue_job_name,
                  'Arguments': {
                  "--raw_bucket_name": destination_bucket,
                  "--processed_bucket_name": processed_bucket,
                  "--s3_dir": src_prefix
                  }                  
                  }
              resp = glue.start_job_run(**glue_start_job_args)
              job_run_id = resp['JobRunId']

              # Wait for the first Glue job to complete
              #waiter = glue.get_waiter('job_run_complete')
              #waiter.wait(JobName=glue_job_name, RunId=job_run_id)
              # throws ValueError: Waiter does not exist: job_run_complete

              time.sleep(60)

              # Check the status of the first Glue job run
              job_run = glue.get_job_run(JobName=glue_job_name, RunId=job_run_id)
              job_run_status = job_run['JobRun']['JobRunState']
              print("job_run_status={}".format(job_run_status))

              if job_run_status == 'RUNNING':
                  print('First Glue job still running.')
                  time.sleep(60)
                  job_run = glue.get_job_run(JobName=glue_job_name, RunId=job_run_id)
                  job_run_status = job_run['JobRun']['JobRunState']     

              # If the first Glue job run was successful, start the second Glue job
              if job_run_status == 'SUCCEEDED':
                  glue_start_job_args2 = {
                  'JobName': glue_job_name2,
                  'Arguments': {
                  "--transformed_bucket_name": transformed_bucket,
                  "--processed_bucket_name": processed_bucket,
                  "--s3_dir": src_prefix
                  }                  
                  }
                  resp2 = glue.start_job_run(**glue_start_job_args2)
                  print('Started second Glue job')
              else:
                  print('First Glue job did not succeed')

              return {
                  'statusCode': 200,
                  'body': 'File copied successfully'
              }
      Runtime: python3.9
      Timeout: 300
      MemorySize: 128

  CombineGlueJobRole:
    Type: 'AWS::IAM::Role'
    Properties:
      RoleName: combine-glue-job-role
      AssumeRolePolicyDocument:
        Version: '2012-10-17'
        Statement:
          - Effect: Allow
            Principal:
              Service: glue.amazonaws.com
            Action: 'sts:AssumeRole'
      Policies:
        - PolicyName: combine-glue-job-policy
          PolicyDocument:
            Version: '2012-10-17'
            Statement:
              - Effect: Allow
                Action:
                  - 'logs:CreateLogGroup'
                  - 'logs:CreateLogStream'
                  - 'logs:PutLogEvents'
                Resource: 'arn:aws:logs:*:*:*'
              - Effect: Allow
                Action:
                  - 's3:GetObject'
                  - 's3:PutObject'
                  - 's3:ListBucket'
                Resource:
                  - !Sub 'arn:aws:s3:::${RawS3Bucket}'
                  - !Sub 'arn:aws:s3:::${RawS3Bucket}/*'
                  - !Sub 'arn:aws:s3:::${ProcessedS3Bucket}'
                  - !Sub 'arn:aws:s3:::${ProcessedS3Bucket}/*'
                  - !Sub 'arn:aws:s3:::${SourceBucketName}'
                  - !Sub 'arn:aws:s3:::${SourceBucketName}/*'

  CombineGlueJob:
    Type: 'AWS::Glue::Job'
    DependsOn:
      - CombineGlueJobRole
      - RawS3Bucket
      - ProcessedS3Bucket
    Properties:
      Name: combine-glue-job
      Role: !GetAtt CombineGlueJobRole.Arn      
      Command:
        Name: 'pythonshell'
        ScriptLocation: !Sub 's3://${SourceBucketName}/combine_csv.py'
        PythonVersion: '3'
      DefaultArguments:
        "--job-language": "python"
        "--raw_bucket_name": !Ref RawS3Bucket
        "--processed_bucket_name": !Ref ProcessedS3Bucket
      
  TransformGlueJobRole:
    Type: "AWS::IAM::Role"
    Properties:
      RoleName: transform-glue-job-role
      AssumeRolePolicyDocument:
        Version: "2012-10-17"
        Statement:
          - Effect: "Allow"
            Principal:
              Service: "glue.amazonaws.com"
            Action: "sts:AssumeRole"
      Policies:
        - PolicyName: "GluePolicy"
          PolicyDocument:
            Version: "2012-10-17"
            Statement:
              - Effect: Allow
                Action:
                  - 'logs:CreateLogGroup'
                  - 'logs:CreateLogStream'
                  - 'logs:PutLogEvents'
                Resource: 'arn:aws:logs:*:*:*'
              - Effect: Allow
                Action:
                  - 's3:GetObject'
                  - 's3:PutObject'
                  - 's3:ListBucket'
                  - 'S3:DeleteObject'
                Resource:
                  - !Sub 'arn:aws:s3:::${TransformedS3Bucket}'
                  - !Sub 'arn:aws:s3:::${TransformedS3Bucket}/*'
                  - !Sub 'arn:aws:s3:::${ProcessedS3Bucket}'
                  - !Sub 'arn:aws:s3:::${ProcessedS3Bucket}/*'
                  - !Sub 'arn:aws:s3:::${SourceBucketName}'
                  - !Sub 'arn:aws:s3:::${SourceBucketName}/*'              

  TransformGlueJob:
    Type: 'AWS::Glue::Job'
    DependsOn:
      - TransformGlueJobRole
      - ProcessedS3Bucket
      - TransformedS3Bucket
    Properties:
      Name: transform-glue-job
      Role: !GetAtt TransformGlueJobRole.Arn
      Command:
        Name: 'glueetl'
        ScriptLocation: !Sub 's3://${SourceBucketName}/csvtoparquet.py'
      DefaultArguments:        
        "--transformed_bucket_name": !Ref TransformedS3Bucket
        "--processed_bucket_name": !Ref ProcessedS3Bucket
        "--enable-job-insights": 'true'
        "--job-language": "python"
        "--enable-metrics": "true"
        "--enable-continuous-cloudwatch-log": "false"
      GlueVersion: "2.0"
      MaxRetries: 0
      Timeout: 60
      WorkerType: "Standard"
      NumberOfWorkers: 1